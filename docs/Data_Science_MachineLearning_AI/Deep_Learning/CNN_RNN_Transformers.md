
# ğŸ§  Deep Learning (CNN, RNN, Transformers) Cheatsheet

## ğŸ”¹ Introduction
Deep Learning is a subfield of machine learning that uses **neural networks with multiple layers** to learn complex patterns from large amounts of data.

This cheatsheet focuses on the three most important neural network architectures:

- ğŸ–¼ï¸ **CNN (Convolutional Neural Networks)** â€” Best for **image processing** and spatial data.
- â³ **RNN (Recurrent Neural Networks)** â€” Designed for **sequential data** like time series and text.
- ğŸ§  **Transformers** â€” The **state-of-the-art architecture** for NLP and beyond, based on **self-attention**.

---

## ğŸ”¹ Convolutional Neural Networks (CNNs)

### âœ… CNN Architecture
| Layer               | Description                                       |
|--------------------|---------------------------------------------------|
| **Convolutional**   | Extracts spatial features using filters/kernels. |
| **Pooling**         | Reduces spatial size (e.g., MaxPooling).         |
| **Fully Connected** | Final classification (Dense layers).             |

ğŸ” **Use Case Example**: Image classification, object detection (e.g., CIFAR-10, ImageNet)

ğŸ’¡ **Tip**: Use `BatchNormalization` after convolutional layers and `Dropout` before dense layers to improve generalization.

### ğŸ“Œ Implementing a CNN in TensorFlow/Keras
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization

model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(64,64,3)),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2,2)),
    Conv2D(64, (3,3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2,2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(10, activation='softmax')
])
```

---

## ğŸ”¹ Recurrent Neural Networks (RNNs)

### âœ… RNN Structure
- Maintains a **hidden state** across time steps.
- Suitable for **time-series, speech, and sequential text data**.

ğŸ§  **Use LSTM or GRU** for better memory and long-sequence performance.

### ğŸ“Œ Simple RNN Implementation
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense

model = Sequential([
    SimpleRNN(50, activation='relu', return_sequences=True, input_shape=(100, 1)),
    SimpleRNN(50, activation='relu'),
    Dense(1, activation='sigmoid')
])
```

### ğŸ“Œ LSTM Example
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

model = Sequential([
    LSTM(64, return_sequences=True, input_shape=(100, 1)),
    LSTM(64),
    Dense(1, activation='sigmoid')
])
```

### âœ… Limitations
- Struggles with **long-term dependencies** due to vanishing gradients.
- RNNs are being replaced by **Transformers** in many NLP tasks.

---

## ğŸ”¹ Transformers (Attention-Based Models)

### âœ… Why Transformers?
- **Replace RNNs** in most NLP applications.
- Use **self-attention** to model relationships between all tokens **in parallel**.

ğŸ¯ In self-attention, each token attends to all other tokens in the sequence to capture context and relevance.

### ğŸ“Œ Transformer Architecture
| Component             | Purpose                                                  |
|-----------------------|----------------------------------------------------------|
| **Self-Attention**    | Measures inter-token relationships.                      |
| **Positional Encoding** | Adds information about token order to input embeddings. |
| **Feedforward Network** | Applies dense layers to token representations.         |
| **Multi-Head Attention** | Captures multiple types of relationships.            |
| **Layer Normalization** | Helps stabilize and speed up training.                 |

### ğŸ“Œ Implementing a Transformer with Hugging Face (BERT)
```python
from transformers import BertTokenizer, TFBertModel

# Load Pretrained Model
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
bert_model = TFBertModel.from_pretrained("bert-base-uncased")
```

### âœ… Applications of Transformers
- **NLP Tasks**: Text classification, machine translation, sentiment analysis.
- **Vision Transformers (ViT)**: Image classification with patch embeddings.
- **AI Coding Tools**: GitHub Copilot, ChatGPT, Codex.

---

## ğŸ”¹ Comparison: CNN vs RNN vs Transformers

| Model         | Best For          | Key Features                           |
|---------------|-------------------|----------------------------------------|
| **CNN**       | Images             | Convolutional layers, feature maps     |
| **RNN**       | Sequential Data    | Memory via hidden states               |
| **Transformers** | NLP, Long Sequences | Self-attention, parallelism         |

---

## ğŸ”¹ Best Practices
- âœ… Use **CNNs for image tasks** instead of RNNs.
- âœ… Prefer **Transformers for NLP** over RNNs/LSTMs.
- âœ… Apply **Transfer Learning** (e.g., BERT, VGG16, ResNet) to boost performance.
- âœ… Use **BatchNormalization** and **Dropout** to regularize deep models.
- âœ… Tune hyperparameters using tools like `keras-tuner` or `Optuna`.
- âœ… Visualize learning using **TensorBoard** or **Weights & Biases**.

---

## ğŸ”¹ Recommended Tools by Task

| Task                    | Suggested Tool/Library          |
|-------------------------|---------------------------------|
| CNN Training            | TensorFlow / Keras / PyTorch    |
| NLP with Transformers   | Hugging Face Transformers       |
| Hyperparameter Tuning   | Keras Tuner, Optuna             |
| Model Monitoring        | TensorBoard, Weights & Biases   |

---

## ğŸ”¹ Final Notes
Deep Learning is evolving rapidly. Stay up-to-date with new architectures like **Diffusion Models**, **ConvNeXt**, or **Mamba**. Always balance **model complexity** with **dataset size**, **computational resources**, and the **specific task** at hand.

ğŸ“ Learn continuously. Experiment often. Deploy responsibly.
